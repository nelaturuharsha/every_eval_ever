{
  "schema_version": "0.0.1",
  "evaluation_id": "hfopenllm_v2/UCLA-AGI_Mistral7B-PairRM-SPPO-Iter2/1762652579.937983",
  "retrieved_timestamp": "1762652579.937984",
  "source_data": [
    "https://open-llm-leaderboard-open-llm-leaderboard.hf.space/api/leaderboard/formatted"
  ],
  "evaluation_source": {
    "evaluation_source_name": "HF Open LLM v2",
    "evaluation_source_type": "leaderboard"
  },
  "source_metadata": {
    "source_organization_name": "Hugging Face",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "UCLA-AGI/Mistral7B-PairRM-SPPO-Iter2",
    "developer": "mistral",
    "inference_platform": "unknown",
    "id": "UCLA-AGI/Mistral7B-PairRM-SPPO-Iter2"
  },
  "evaluation_results": [
    {
      "evaluation_name": "IFEval",
      "metric_config": {
        "evaluation_description": "Accuracy on IFEval",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.4445848127413041
      }
    },
    {
      "evaluation_name": "BBH",
      "metric_config": {
        "evaluation_description": "Accuracy on BBH",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.4465719945610438
      }
    },
    {
      "evaluation_name": "MATH Level 5",
      "metric_config": {
        "evaluation_description": "Exact Match on MATH Level 5",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.02190332326283988
      }
    },
    {
      "evaluation_name": "GPQA",
      "metric_config": {
        "evaluation_description": "Accuracy on GPQA",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.28859060402684567
      }
    },
    {
      "evaluation_name": "MUSR",
      "metric_config": {
        "evaluation_description": "Accuracy on MUSR",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.40854166666666664
      }
    },
    {
      "evaluation_name": "MMLU-PRO",
      "metric_config": {
        "evaluation_description": "Accuracy on MMLU-PRO",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.2677027925531915
      }
    }
  ],
  "additional_details": {
    "precision": "bfloat16",
    "architecture": "MistralForCausalLM",
    "params_billions": 7.242
  }
}