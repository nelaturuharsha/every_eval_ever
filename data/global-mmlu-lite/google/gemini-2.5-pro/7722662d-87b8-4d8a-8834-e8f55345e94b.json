{
  "schema_version": "0.0.1",
  "evaluation_id": "global-mmlu-lite/gemini-2.5-pro/1762357502.158017",
  "retrieved_timestamp": "1762357502.158017",
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "gemini-2.5-pro",
    "developer": "Google",
    "inference_platform": "unknown",
    "id": "google/gemini-2.5-pro"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "evaluation_description": "Accuracy on Global MMLU Lite",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.93234375
      }
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "evaluation_description": "Accuracy on Arabic",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9475,
        "confidence_interval": 0.0218568391591684
      }
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "evaluation_description": "Accuracy on English",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9275,
        "confidence_interval": 0.0254123049217328
      }
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "evaluation_description": "Accuracy on Bengali",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9275,
        "confidence_interval": 0.0254123049217328
      }
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "evaluation_description": "Accuracy on German",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.93,
        "confidence_interval": 0.0250039481496016
      }
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "evaluation_description": "Accuracy on French",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9425,
        "confidence_interval": 0.0228135408783901
      }
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "evaluation_description": "Accuracy on Hindi",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9275,
        "confidence_interval": 0.0254123049217328
      }
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "evaluation_description": "Accuracy on Indonesian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.925,
        "confidence_interval": 0.0258118773864695
      }
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "evaluation_description": "Accuracy on Italian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.935,
        "confidence_interval": 0.0241590904127041
      }
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "evaluation_description": "Accuracy on Japanese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9375,
        "confidence_interval": 0.0237215870977811
      }
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "evaluation_description": "Accuracy on Korean",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9275,
        "confidence_interval": 0.0254123049217328
      }
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "evaluation_description": "Accuracy on Portuguese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.93,
        "confidence_interval": 0.0250039481496016
      }
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "evaluation_description": "Accuracy on Spanish",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.94,
        "confidence_interval": 0.0232732828307025
      }
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "evaluation_description": "Accuracy on Swahili",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9375,
        "confidence_interval": 0.0237215870977811
      }
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "evaluation_description": "Accuracy on Yoruba",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.925,
        "confidence_interval": 0.0258118773864695
      }
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "evaluation_description": "Accuracy on Chinese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9275,
        "confidence_interval": 0.0254123049217328
      }
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "evaluation_description": "Accuracy on Burmese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.93,
        "confidence_interval": 0.0250039481496016
      }
    }
  ],
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-arabic/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-bengali/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-burmese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-chinese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-english/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-french/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-german/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-hindi/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-indonesian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-italian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-japanese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-korean/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-portuguese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-spanish/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-swahili/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-yoruba/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite/versions/1"
  ],
  "evaluation_source": {
    "evaluation_source_name": "Global MMLU Lite",
    "evaluation_source_type": "leaderboard"
  }
}