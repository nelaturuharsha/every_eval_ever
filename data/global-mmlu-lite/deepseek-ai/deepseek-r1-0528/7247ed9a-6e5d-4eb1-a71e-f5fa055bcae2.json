{
  "schema_version": "0.0.1",
  "evaluation_id": "global-mmlu-lite/deepseek-r1-0528/1762357502.158017",
  "retrieved_timestamp": "1762357502.158017",
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "deepseek-r1-0528",
    "developer": "DeepSeek",
    "inference_platform": "unknown",
    "id": "deepseek-ai/deepseek-r1-0528"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "evaluation_description": "Accuracy on Global MMLU Lite",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.674375
      }
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "evaluation_description": "Accuracy on Arabic",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6825,
        "confidence_interval": 0.0456185301529649
      }
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "evaluation_description": "Accuracy on English",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.715,
        "confidence_interval": 0.0442378025897236
      }
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "evaluation_description": "Accuracy on Bengali",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.655,
        "confidence_interval": 0.0465852352416072
      }
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "evaluation_description": "Accuracy on German",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6375,
        "confidence_interval": 0.0471099014100216
      }
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "evaluation_description": "Accuracy on French",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6925,
        "confidence_interval": 0.0452220810763167
      }
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "evaluation_description": "Accuracy on Hindi",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6475,
        "confidence_interval": 0.046818505067596
      }
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "evaluation_description": "Accuracy on Indonesian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.655,
        "confidence_interval": 0.0465852352416072
      }
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "evaluation_description": "Accuracy on Italian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6775,
        "confidence_interval": 0.0458076069884696
      }
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "evaluation_description": "Accuracy on Japanese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.7725,
        "confidence_interval": 0.0410826112430601
      }
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "evaluation_description": "Accuracy on Korean",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6575,
        "confidence_interval": 0.0465046373306654
      }
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "evaluation_description": "Accuracy on Portuguese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.635,
        "confidence_interval": 0.0471792888396587
      }
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "evaluation_description": "Accuracy on Spanish",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.7175,
        "confidence_interval": 0.0441202814428089
      }
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "evaluation_description": "Accuracy on Swahili",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6775,
        "confidence_interval": 0.0458076069884696
      }
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "evaluation_description": "Accuracy on Yoruba",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.77,
        "confidence_interval": 0.0412408279846843
      }
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "evaluation_description": "Accuracy on Chinese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.5075,
        "confidence_interval": 0.0489935869046875
      }
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "evaluation_description": "Accuracy on Burmese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.69,
        "confidence_interval": 0.0453235049876571
      }
    }
  ],
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-arabic/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-bengali/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-burmese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-chinese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-english/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-french/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-german/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-hindi/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-indonesian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-italian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-japanese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-korean/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-portuguese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-spanish/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-swahili/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-yoruba/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite/versions/1"
  ],
  "evaluation_source": {
    "evaluation_source_name": "Global MMLU Lite",
    "evaluation_source_type": "leaderboard"
  }
}