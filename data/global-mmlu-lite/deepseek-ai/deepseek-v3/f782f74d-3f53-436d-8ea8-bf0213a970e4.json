{
  "schema_version": "0.0.1",
  "evaluation_id": "global-mmlu-lite/deepseek-v3/1762357502.158017",
  "retrieved_timestamp": "1762357502.158017",
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "deepseek-v3",
    "developer": "DeepSeek",
    "inference_platform": "unknown",
    "id": "deepseek-ai/deepseek-v3"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "evaluation_description": "Accuracy on Global MMLU Lite",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.569767441860465
      }
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "evaluation_description": "Accuracy on Arabic",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6372093023255814,
        "confidence_interval": 0.0642684737023654
      }
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "evaluation_description": "Accuracy on English",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.5395348837209303,
        "confidence_interval": 0.0666249648729137
      }
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "evaluation_description": "Accuracy on Bengali",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.5069767441860465,
        "confidence_interval": 0.0668277105287728
      }
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "evaluation_description": "Accuracy on German",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.4930232558139535,
        "confidence_interval": 0.0668277105287728
      }
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "evaluation_description": "Accuracy on French",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.5255813953488372,
        "confidence_interval": 0.0667466861682081
      }
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "evaluation_description": "Accuracy on Hindi",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.4697674418604651,
        "confidence_interval": 0.0667119313175475
      }
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "evaluation_description": "Accuracy on Indonesian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.4697674418604651,
        "confidence_interval": 0.0667119313175475
      }
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "evaluation_description": "Accuracy on Italian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6,
        "confidence_interval": 0.0654838917514404
      }
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "evaluation_description": "Accuracy on Japanese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.5255813953488372,
        "confidence_interval": 0.0667466861682081
      }
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "evaluation_description": "Accuracy on Korean",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.5023255813953489,
        "confidence_interval": 0.0668334942243593
      }
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "evaluation_description": "Accuracy on Portuguese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.5209302325581395,
        "confidence_interval": 0.0667756347258289
      }
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "evaluation_description": "Accuracy on Spanish",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6465116279069767,
        "confidence_interval": 0.0639005501784673
      }
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "evaluation_description": "Accuracy on Swahili",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.6558139534883721,
        "confidence_interval": 0.0635061547514093
      }
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "evaluation_description": "Accuracy on Yoruba",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8232558139534883,
        "confidence_interval": 0.05098810043927
      }
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "evaluation_description": "Accuracy on Chinese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.5581395348837209,
        "confidence_interval": 0.0663808526916943
      }
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "evaluation_description": "Accuracy on Burmese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.641860465116279,
        "confidence_interval": 0.0640877916644912
      }
    }
  ],
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-arabic/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-bengali/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-burmese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-chinese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-english/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-french/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-german/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-hindi/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-indonesian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-italian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-japanese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-korean/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-portuguese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-spanish/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-swahili/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-yoruba/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite/versions/1"
  ],
  "evaluation_source": {
    "evaluation_source_name": "Global MMLU Lite",
    "evaluation_source_type": "leaderboard"
  }
}