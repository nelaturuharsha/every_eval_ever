{
  "schema_version": "0.0.1",
  "evaluation_id": "global-mmlu-lite/grok-4-0709/1762357502.158017",
  "retrieved_timestamp": "1762357502.158017",
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "grok-4-0709",
    "developer": "xAI",
    "inference_platform": "unknown",
    "id": "xai/grok-4-0709"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "evaluation_description": "Accuracy on Global MMLU Lite",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9110465116279071
      }
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "evaluation_description": "Accuracy on Arabic",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8744186046511628,
        "confidence_interval": 0.0442946432133054
      }
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "evaluation_description": "Accuracy on English",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9488372093023256,
        "confidence_interval": 0.0294511208251068
      }
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "evaluation_description": "Accuracy on Bengali",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9302325581395348,
        "confidence_interval": 0.0340526589273159
      }
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "evaluation_description": "Accuracy on German",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9302325581395348,
        "confidence_interval": 0.0340526589273159
      }
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "evaluation_description": "Accuracy on French",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8744186046511628,
        "confidence_interval": 0.0442946432133054
      }
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "evaluation_description": "Accuracy on Hindi",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8697674418604651,
        "confidence_interval": 0.0449873301607808
      }
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "evaluation_description": "Accuracy on Indonesian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8976744186046511,
        "confidence_interval": 0.0405116932945102
      }
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "evaluation_description": "Accuracy on Italian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9255813953488372,
        "confidence_interval": 0.0350814011445002
      }
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "evaluation_description": "Accuracy on Japanese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8976744186046511,
        "confidence_interval": 0.0405116932945102
      }
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "evaluation_description": "Accuracy on Korean",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9209302325581395,
        "confidence_interval": 0.0360701091872836
      }
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "evaluation_description": "Accuracy on Portuguese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9069767441860463,
        "confidence_interval": 0.0388260048682084
      }
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "evaluation_description": "Accuracy on Spanish",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9255813953488372,
        "confidence_interval": 0.0350814011445002
      }
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "evaluation_description": "Accuracy on Swahili",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.944186046511628,
        "confidence_interval": 0.030685205995338
      }
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "evaluation_description": "Accuracy on Yoruba",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9255813953488372,
        "confidence_interval": 0.0350814011445002
      }
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "evaluation_description": "Accuracy on Chinese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8697674418604651,
        "confidence_interval": 0.0449873301607808
      }
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "evaluation_description": "Accuracy on Burmese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9348837209302324,
        "confidence_interval": 0.03298013641662
      }
    }
  ],
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-arabic/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-bengali/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-burmese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-chinese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-english/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-french/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-german/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-hindi/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-indonesian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-italian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-japanese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-korean/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-portuguese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-spanish/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-swahili/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-yoruba/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite/versions/1"
  ],
  "evaluation_source": {
    "evaluation_source_name": "Global MMLU Lite",
    "evaluation_source_type": "leaderboard"
  }
}