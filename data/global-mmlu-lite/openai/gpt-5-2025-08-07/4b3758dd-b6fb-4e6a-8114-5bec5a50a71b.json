{
  "schema_version": "0.0.1",
  "evaluation_id": "global-mmlu-lite/gpt-5-2025-08-07/1762357502.158017",
  "retrieved_timestamp": "1762357502.158017",
  "source_metadata": {
    "source_organization_name": "Cohere Labs",
    "evaluator_relationship": "third_party"
  },
  "model_info": {
    "name": "gpt-5-2025-08-07",
    "developer": "OpenAI",
    "inference_platform": "unknown",
    "id": "openai/gpt-5-2025-08-07"
  },
  "evaluation_results": [
    {
      "evaluation_name": "Global MMLU Lite",
      "metric_config": {
        "evaluation_description": "Accuracy on Global MMLU Lite",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8895312499999999
      }
    },
    {
      "evaluation_name": "Arabic",
      "metric_config": {
        "evaluation_description": "Accuracy on Arabic",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8925,
        "confidence_interval": 0.0303547345865505
      }
    },
    {
      "evaluation_name": "English",
      "metric_config": {
        "evaluation_description": "Accuracy on English",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8725,
        "confidence_interval": 0.0326855581520567
      }
    },
    {
      "evaluation_name": "Bengali",
      "metric_config": {
        "evaluation_description": "Accuracy on Bengali",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9,
        "confidence_interval": 0.0293994597681008
      }
    },
    {
      "evaluation_name": "German",
      "metric_config": {
        "evaluation_description": "Accuracy on German",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.91,
        "confidence_interval": 0.0280452971732717
      }
    },
    {
      "evaluation_name": "French",
      "metric_config": {
        "evaluation_description": "Accuracy on French",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9075,
        "confidence_interval": 0.0283930651251164
      }
    },
    {
      "evaluation_name": "Hindi",
      "metric_config": {
        "evaluation_description": "Accuracy on Hindi",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.865,
        "confidence_interval": 0.0334882947381079
      }
    },
    {
      "evaluation_name": "Indonesian",
      "metric_config": {
        "evaluation_description": "Accuracy on Indonesian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.795,
        "confidence_interval": 0.0395620320289107
      }
    },
    {
      "evaluation_name": "Italian",
      "metric_config": {
        "evaluation_description": "Accuracy on Italian",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9075,
        "confidence_interval": 0.0283930651251164
      }
    },
    {
      "evaluation_name": "Japanese",
      "metric_config": {
        "evaluation_description": "Accuracy on Japanese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8875,
        "confidence_interval": 0.0309655314070612
      }
    },
    {
      "evaluation_name": "Korean",
      "metric_config": {
        "evaluation_description": "Accuracy on Korean",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.915,
        "confidence_interval": 0.0273299039414468
      }
    },
    {
      "evaluation_name": "Portuguese",
      "metric_config": {
        "evaluation_description": "Accuracy on Portuguese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.8875,
        "confidence_interval": 0.0309655314070612
      }
    },
    {
      "evaluation_name": "Spanish",
      "metric_config": {
        "evaluation_description": "Accuracy on Spanish",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.905,
        "confidence_interval": 0.0287345359327925
      }
    },
    {
      "evaluation_name": "Swahili",
      "metric_config": {
        "evaluation_description": "Accuracy on Swahili",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.865,
        "confidence_interval": 0.0334882947381079
      }
    },
    {
      "evaluation_name": "Yoruba",
      "metric_config": {
        "evaluation_description": "Accuracy on Yoruba",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.9125,
        "confidence_interval": 0.0276909948229923
      }
    },
    {
      "evaluation_name": "Chinese",
      "metric_config": {
        "evaluation_description": "Accuracy on Chinese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.895,
        "confidence_interval": 0.0300416832365769
      }
    },
    {
      "evaluation_name": "Burmese",
      "metric_config": {
        "evaluation_description": "Accuracy on Burmese",
        "lower_is_better": false,
        "score_type": "continuous",
        "min_score": 0,
        "max_score": 1
      },
      "score_details": {
        "score": 0.915,
        "confidence_interval": 0.0273299039414468
      }
    }
  ],
  "source_data": [
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-arabic/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-bengali/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-burmese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-chinese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-english/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-french/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-german/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-hindi/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-indonesian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-italian/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-japanese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-korean/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-portuguese/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-spanish/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-swahili/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite-yoruba/versions/1",
    "https://www.kaggle.com/benchmarks/cohere-labs/global-mmlu-lite/versions/1"
  ],
  "evaluation_source": {
    "evaluation_source_name": "Global MMLU Lite",
    "evaluation_source_type": "leaderboard"
  }
}